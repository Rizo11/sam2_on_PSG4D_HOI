{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Segment Anything Model 2 (SAM 2)**\n",
        "![SAM2](https://github.com/ibaiGorordo/ONNX-SAM2-Segment-Anything/raw/main/doc/img/sam2_annotation.gif)"
      ],
      "metadata": {
        "id": "qlsCAu5JBIcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation !!Requires GPU runtime!!"
      ],
      "metadata": {
        "id": "TVR_BlMZD4XJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBSQnXVFAiwg",
        "outputId": "364ce834-bdd8-4870-a4b6-293b42b4c844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'segment-anything-2' already exists and is not an empty directory.\n",
            "/content/segment-anything-2\n",
            "Obtaining file:///content/segment-anything-2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.20.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (4.67.1)\n",
            "Requirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (1.3.2)\n",
            "Requirement already satisfied: iopath>=0.1.10 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (0.1.10)\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from SAM-2==1.0) (11.1.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->SAM-2==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.1->SAM-2==1.0) (1.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.1->SAM-2==1.0) (3.0.2)\n",
            "Building wheels for collected packages: SAM-2\n",
            "  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SAM-2: filename=SAM_2-1.0-0.editable-cp311-cp311-linux_x86_64.whl size=13812 sha256=fd7249f8c158beb5d0360dd2a4430a9cec86a40c950f46f13958c0496256bc28\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-45422yps/wheels/79/7c/e1/0da3f0d4adfcc74ea4d1578b1a77a5a1647d6dc06af87a30e7\n",
            "Successfully built SAM-2\n",
            "Installing collected packages: SAM-2\n",
            "  Attempting uninstall: SAM-2\n",
            "    Found existing installation: SAM-2 1.0\n",
            "    Uninstalling SAM-2-1.0:\n",
            "      Successfully uninstalled SAM-2-1.0\n",
            "Successfully installed SAM-2-1.0\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.11/dist-packages (0.1.0)\n",
            "Requirement already satisfied: onnxsim in /usr/local/lib/python3.11/dist-packages (0.4.36)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.20.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from onnxscript) (4.12.2)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.11/dist-packages (from onnxscript) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxscript) (24.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from onnxsim) (13.9.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.1.24)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (2.18.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd /content/segment-anything-2\n",
        "!pip3 install -e .\n",
        "!pip3 install onnx onnxscript onnxsim onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/checkpoints\n",
        "!./download_ckpts.sh"
      ],
      "metadata": {
        "id": "Vtld9UUcAxH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa5b3b6-b31c-4d45-ff5c-7394c37b8ad0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2/checkpoints\n",
            "Downloading sam2.1_hiera_tiny.pt checkpoint...\n",
            "--2025-02-11 22:35:39--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.70, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 156008466 (149M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2.1_hiera_tiny.pt’\n",
            "\n",
            "sam2.1_hiera_tiny.p 100%[===================>] 148.78M   226MB/s    in 0.7s    \n",
            "\n",
            "2025-02-11 22:35:40 (226 MB/s) - ‘sam2.1_hiera_tiny.pt’ saved [156008466/156008466]\n",
            "\n",
            "Downloading sam2.1_hiera_small.pt checkpoint...\n",
            "--2025-02-11 22:35:40--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.70, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 184416285 (176M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2.1_hiera_small.pt’\n",
            "\n",
            "sam2.1_hiera_small. 100%[===================>] 175.87M   242MB/s    in 0.7s    \n",
            "\n",
            "2025-02-11 22:35:40 (242 MB/s) - ‘sam2.1_hiera_small.pt’ saved [184416285/184416285]\n",
            "\n",
            "Downloading sam2.1_hiera_base_plus.pt checkpoint...\n",
            "--2025-02-11 22:35:40--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.70, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 323606802 (309M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2.1_hiera_base_plus.pt’\n",
            "\n",
            "sam2.1_hiera_base_p 100%[===================>] 308.62M  94.7MB/s    in 5.2s    \n",
            "\n",
            "2025-02-11 22:35:46 (59.4 MB/s) - ‘sam2.1_hiera_base_plus.pt’ saved [323606802/323606802]\n",
            "\n",
            "Downloading sam2.1_hiera_large.pt checkpoint...\n",
            "--2025-02-11 22:35:46--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.70, 13.227.219.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 898083611 (856M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘sam2.1_hiera_large.pt’\n",
            "\n",
            "sam2.1_hiera_large. 100%[===================>] 856.48M   210MB/s    in 7.3s    \n",
            "\n",
            "2025-02-11 22:35:53 (118 MB/s) - ‘sam2.1_hiera_large.pt’ saved [898083611/898083611]\n",
            "\n",
            "All checkpoints are downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/\n",
        "from typing import Optional, Tuple, Any\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import trunc_normal_\n",
        "\n",
        "\n",
        "from sam2.modeling.sam2_base import SAM2Base\n",
        "\n",
        "class SAM2ImageEncoder(nn.Module):\n",
        "    def __init__(self, sam_model: SAM2Base) -> None:\n",
        "        super().__init__()\n",
        "        self.model = sam_model\n",
        "        self.image_encoder = sam_model.image_encoder\n",
        "        self.no_mem_embed = sam_model.no_mem_embed\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[Any, Any, Any]:\n",
        "        backbone_out = self.image_encoder(x)\n",
        "        backbone_out[\"backbone_fpn\"][0] = self.model.sam_mask_decoder.conv_s0(\n",
        "            backbone_out[\"backbone_fpn\"][0]\n",
        "        )\n",
        "        backbone_out[\"backbone_fpn\"][1] = self.model.sam_mask_decoder.conv_s1(\n",
        "            backbone_out[\"backbone_fpn\"][1]\n",
        "        )\n",
        "\n",
        "        feature_maps = backbone_out[\"backbone_fpn\"][-self.model.num_feature_levels:]\n",
        "        vision_pos_embeds = backbone_out[\"vision_pos_enc\"][-self.model.num_feature_levels:]\n",
        "\n",
        "        feat_sizes = [(x.shape[-2], x.shape[-1]) for x in vision_pos_embeds]\n",
        "\n",
        "        # flatten NxCxHxW to HWxNxC\n",
        "        vision_feats = [x.flatten(2).permute(2, 0, 1) for x in feature_maps]\n",
        "        vision_pos_embeds = [x.flatten(2).permute(2, 0, 1) for x in vision_pos_embeds]\n",
        "\n",
        "        vision_feats[-1] = vision_feats[-1] + self.no_mem_embed\n",
        "\n",
        "        feats = [feat.permute(1, 2, 0).reshape(1, -1, *feat_size)\n",
        "                 for feat, feat_size in zip(vision_feats[::-1], feat_sizes[::-1])][::-1]\n",
        "\n",
        "        return feats[0], feats[1], feats[2]\n",
        "\n",
        "\n",
        "class SAM2ImageDecoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            sam_model: SAM2Base,\n",
        "            multimask_output: bool\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.mask_decoder = sam_model.sam_mask_decoder\n",
        "        self.prompt_encoder = sam_model.sam_prompt_encoder\n",
        "        self.model = sam_model\n",
        "        self.multimask_output = multimask_output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(\n",
        "            self,\n",
        "            image_embed: torch.Tensor,\n",
        "            high_res_feats_0: torch.Tensor,\n",
        "            high_res_feats_1: torch.Tensor,\n",
        "            point_coords: torch.Tensor,\n",
        "            point_labels: torch.Tensor,\n",
        "            mask_input: torch.Tensor,\n",
        "            has_mask_input: torch.Tensor,\n",
        "            img_size: torch.Tensor\n",
        "    ):\n",
        "        sparse_embedding = self._embed_points(point_coords, point_labels)\n",
        "        self.sparse_embedding = sparse_embedding\n",
        "        dense_embedding = self._embed_masks(mask_input, has_mask_input)\n",
        "\n",
        "        high_res_feats = [high_res_feats_0, high_res_feats_1]\n",
        "        image_embed = image_embed\n",
        "\n",
        "        masks, iou_predictions, _, _ = self.mask_decoder.predict_masks(\n",
        "            image_embeddings=image_embed,\n",
        "            image_pe=self.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embedding,\n",
        "            dense_prompt_embeddings=dense_embedding,\n",
        "            repeat_image=False,\n",
        "            high_res_features=high_res_feats,\n",
        "        )\n",
        "\n",
        "        if self.multimask_output:\n",
        "            masks = masks[:, 1:, :, :]\n",
        "            iou_predictions = iou_predictions[:, 1:]\n",
        "        else:\n",
        "            masks, iou_predictions = self.mask_decoder._dynamic_multimask_via_stability(masks, iou_predictions)\n",
        "\n",
        "        masks = torch.clamp(masks, -32.0, 32.0)\n",
        "        print(masks.shape, iou_predictions.shape)\n",
        "\n",
        "        masks = F.interpolate(masks, (img_size[0], img_size[1]), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        return masks, iou_predictions\n",
        "\n",
        "    def _embed_points(self, point_coords: torch.Tensor, point_labels: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        point_coords = point_coords + 0.5\n",
        "\n",
        "        padding_point = torch.zeros((point_coords.shape[0], 1, 2), device=point_coords.device)\n",
        "        padding_label = -torch.ones((point_labels.shape[0], 1), device=point_labels.device)\n",
        "        point_coords = torch.cat([point_coords, padding_point], dim=1)\n",
        "        point_labels = torch.cat([point_labels, padding_label], dim=1)\n",
        "\n",
        "        point_coords[:, :, 0] = point_coords[:, :, 0] / self.model.image_size\n",
        "        point_coords[:, :, 1] = point_coords[:, :, 1] / self.model.image_size\n",
        "\n",
        "        point_embedding = self.prompt_encoder.pe_layer._pe_encoding(point_coords)\n",
        "        point_labels = point_labels.unsqueeze(-1).expand_as(point_embedding)\n",
        "\n",
        "        point_embedding = point_embedding * (point_labels != -1)\n",
        "        point_embedding = point_embedding + self.prompt_encoder.not_a_point_embed.weight * (\n",
        "                point_labels == -1\n",
        "        )\n",
        "\n",
        "        for i in range(self.prompt_encoder.num_point_embeddings):\n",
        "            point_embedding = point_embedding + self.prompt_encoder.point_embeddings[i].weight * (point_labels == i)\n",
        "\n",
        "        return point_embedding\n",
        "\n",
        "    def _embed_masks(self, input_mask: torch.Tensor, has_mask_input: torch.Tensor) -> torch.Tensor:\n",
        "        mask_embedding = has_mask_input * self.prompt_encoder.mask_downscaling(input_mask)\n",
        "        mask_embedding = mask_embedding + (\n",
        "                1 - has_mask_input\n",
        "        ) * self.prompt_encoder.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
        "        return mask_embedding"
      ],
      "metadata": {
        "id": "xQAznoeDIyak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26315893-6607-43d3-8160-a6f8a7005ac3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select  model parameters"
      ],
      "metadata": {
        "id": "DGJ8EjKE7zd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = 'sam2_hiera_tiny' #@param [\"sam2_hiera_tiny\", \"sam2_hiera_small\", \"sam2_hiera_large\", \"sam2_hiera_base_plus\"]\n",
        "# input_size = 768 #@param {type:\"slider\", min:160, max:4102, step:8}\n",
        "input_size = 1024 # Bad output if anything else (for now)\n",
        "multimask_output = False\n",
        "\n",
        "if model_type == \"sam2_hiera_tiny\":\n",
        "    model_cfg = \"sam2_hiera_t.yaml\"\n",
        "elif model_type == \"sam2_hiera_small\":\n",
        "    model_cfg = \"sam2_hiera_s.yaml\"\n",
        "elif model_type == \"sam2_hiera_base_plus\":\n",
        "    model_cfg = \"sam2_hiera_b+.yaml\"\n",
        "else:\n",
        "    model_cfg = \"sam2_hiera_l.yaml\"\n"
      ],
      "metadata": {
        "id": "K-Ll5Iwh7428"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Encoder"
      ],
      "metadata": {
        "id": "t46_lYeIy0ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/segment-anything-2/\n",
        "import torch\n",
        "from sam2.build_sam import build_sam2\n",
        "\n",
        "sam2_checkpoint = f\"/content/segment-anything-2/checkpoints/sam2.1_hiera_tiny.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cpu\")\n",
        "\n",
        "img=torch.randn(1, 3, input_size, input_size).cpu()\n",
        "\n",
        "sam2_encoder = SAM2ImageEncoder(sam2_model).cpu()\n",
        "high_res_feats_0, high_res_feats_1, image_embed = sam2_encoder(img)\n",
        "print(high_res_feats_0.shape)\n",
        "print(high_res_feats_1.shape)\n",
        "print(image_embed.shape)\n",
        "\n",
        "torch.onnx.export(sam2_encoder,\n",
        "                  img,\n",
        "                  f\"{model_type}_encoder.onnx\",\n",
        "                  export_params=True,\n",
        "                  opset_version=17,\n",
        "                  do_constant_folding=True,\n",
        "                  input_names = ['image'],\n",
        "                  output_names = ['high_res_feats_0', 'high_res_feats_1', 'image_embed']\n",
        "                )"
      ],
      "metadata": {
        "id": "IgHx4lbupej-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5176aab-d906-47f3-e1bb-76a18735df5b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2\n",
            "torch.Size([1, 32, 256, 256])\n",
            "torch.Size([1, 64, 128, 128])\n",
            "torch.Size([1, 256, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/segment-anything-2/sam2/modeling/backbones/utils.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pad_h > 0 or pad_w > 0:\n",
            "/content/segment-anything-2/sam2/modeling/backbones/utils.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if Hp > H or Wp > W:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Decoder"
      ],
      "metadata": {
        "id": "JX1N64Y6y2-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/\n",
        "\n",
        "\n",
        "sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).cpu()\n",
        "\n",
        "embed_dim = sam2_model.sam_prompt_encoder.embed_dim\n",
        "embed_size = (sam2_model.image_size // sam2_model.backbone_stride, sam2_model.image_size // sam2_model.backbone_stride)\n",
        "mask_input_size = [4 * x for x in embed_size]\n",
        "print(embed_dim, embed_size, mask_input_size)\n",
        "\n",
        "point_coords = torch.randint(low=0, high=input_size, size=(1, 5, 2), dtype=torch.float)\n",
        "point_labels = torch.randint(low=0, high=1, size=(1, 5), dtype=torch.float)\n",
        "mask_input = torch.randn(1, 1, *mask_input_size, dtype=torch.float)\n",
        "has_mask_input = torch.tensor([1], dtype=torch.float)\n",
        "orig_im_size = torch.tensor([input_size, input_size], dtype=torch.int32)\n",
        "\n",
        "masks, scores = sam2_decoder(image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size)\n",
        "\n",
        "\n",
        "torch.onnx.export(sam2_decoder,\n",
        "                  (image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size),\n",
        "                  \"decoder.onnx\",\n",
        "                  export_params=True,\n",
        "                  opset_version=16,\n",
        "                  do_constant_folding=True,\n",
        "                  input_names = ['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'point_coords', 'point_labels', 'mask_input', 'has_mask_input', 'orig_im_size'],\n",
        "                  output_names = ['masks', 'iou_predictions'],\n",
        "                  dynamic_axes = {\"point_coords\": {0: \"num_labels\", 1: \"num_points\"},\n",
        "                                  \"point_labels\": {0: \"num_labels\", 1: \"num_points\"},\n",
        "                                  \"mask_input\": {0: \"num_labels\"},\n",
        "                                  \"has_mask_input\": {0: \"num_labels\"}\n",
        "                  }\n",
        "                )\n"
      ],
      "metadata": {
        "id": "KKqrn0sHaQYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44d8c16-f02c-4257-b107-407c56596f83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2\n",
            "256 (64, 64) [256, 256]\n",
            "torch.Size([1, 1, 256, 256]) torch.Size([1, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/segment-anything-2/sam2/modeling/sam/mask_decoder.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert image_embeddings.shape[0] == tokens.shape[0]\n",
            "/content/segment-anything-2/sam2/modeling/sam/mask_decoder.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  image_pe.size(0) == 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 256, 256]) torch.Size([1, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 16 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplify models"
      ],
      "metadata": {
        "id": "dMgwgcWO18hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/\n",
        "!onnxsim {model_type}_encoder.onnx {model_type}_encoder.onnx\n",
        "!onnxsim decoder.onnx decoder.onnx"
      ],
      "metadata": {
        "id": "w4nMB2XD1-gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001bb7b2-af58-4d5f-9b49-dfdb7cea4d49"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2\n",
            "\u001b[1;35mYour model contains \"Tile\" ops or/and \"ConstantOfShape\" ops. Folding these ops can make the \u001b[0m\n",
            "\u001b[1;35msimplified model much larger. If it is not expected, please specify \"--no-large-tensor\" (which will \u001b[0m\n",
            "\u001b[1;35mlose some optimization chances)\u001b[0m\n",
            "Simplifying\u001b[33m...\u001b[0m\n",
            "Finish! Here is the difference:\n",
            "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
            "│ Abs                │ 1              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Add                │ 127            │ \u001b[1;32m90              \u001b[0m │\n",
            "│ Cast               │ 155            │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Concat             │ 119            │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Constant           │ 1205           │ \u001b[1;32m218             \u001b[0m │\n",
            "│ ConstantOfShape    │ 11             │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Conv               │ 7              │ 7                │\n",
            "│ Cos                │ 6              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Div                │ 91             │ \u001b[1;32m12              \u001b[0m │\n",
            "│ Erf                │ 12             │ 12               │\n",
            "│ Expand             │ 8              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Gather             │ 117            │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Greater            │ 1              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Identity           │ 2              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ If                 │ 2              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ LayerNormalization │ 24             │ 24               │\n",
            "│ Less               │ 1              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ MatMul             │ 75             │ 75               │\n",
            "│ MaxPool            │ 6              │ 6                │\n",
            "│ Mod                │ 48             │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Mul                │ 78             │ \u001b[1;32m48              \u001b[0m │\n",
            "│ Pad                │ 5              │ 5                │\n",
            "│ Range              │ 6              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Reshape            │ 97             │ \u001b[1;32m70              \u001b[0m │\n",
            "│ Resize             │ 2              │ \u001b[1;32m1               \u001b[0m │\n",
            "│ Shape              │ 144            │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Sin                │ 6              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Size               │ 1              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Slice              │ 54             │ \u001b[1;32m5               \u001b[0m │\n",
            "│ Softmax            │ 12             │ 12               │\n",
            "│ Split              │ 12             │ 12               │\n",
            "│ Sqrt               │ 36             │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Squeeze            │ 36             │ 36               │\n",
            "│ Sub                │ 30             │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Tile               │ 7              │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Transpose          │ 94             │ \u001b[1;32m85              \u001b[0m │\n",
            "│ Unsqueeze          │ 264            │ \u001b[1;32m0               \u001b[0m │\n",
            "│ Model Size         │ 104.4MiB       │ 128.0MiB         │\n",
            "└────────────────────┴────────────────┴──────────────────┘\n",
            "\u001b[1;35mYour model contains \"Tile\" ops or/and \"ConstantOfShape\" ops. Folding these ops can make the \u001b[0m\n",
            "\u001b[1;35msimplified model much larger. If it is not expected, please specify \"--no-large-tensor\" (which will \u001b[0m\n",
            "\u001b[1;35mlose some optimization chances)\u001b[0m\n",
            "Simplifying\u001b[33m...\u001b[0m\n",
            "Finish! Here is the difference:\n",
            "┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
            "│ Add             │ 91             │ \u001b[1;32m90              \u001b[0m │\n",
            "│ ArgMax          │ 1              │ 1                │\n",
            "│ Cast            │ 72             │ \u001b[1;32m18              \u001b[0m │\n",
            "│ Clip            │ 1              │ 1                │\n",
            "│ Concat          │ 55             │ \u001b[1;32m47              \u001b[0m │\n",
            "│ Constant        │ 580            │ \u001b[1;32m179             \u001b[0m │\n",
            "│ ConstantOfShape │ 9              │ \u001b[1;32m2               \u001b[0m │\n",
            "│ Conv            │ 3              │ 3                │\n",
            "│ ConvTranspose   │ 2              │ 2                │\n",
            "│ Cos             │ 2              │ \u001b[1;32m1               \u001b[0m │\n",
            "│ Div             │ 47             │ \u001b[1;32m26              \u001b[0m │\n",
            "│ Equal           │ 13             │ \u001b[1;32m8               \u001b[0m │\n",
            "│ Erf             │ 4              │ 4                │\n",
            "│ Expand          │ 12             │ 12               │\n",
            "│ Flatten         │ 2              │ 2                │\n",
            "│ Gather          │ 125            │ \u001b[1;32m94              \u001b[0m │\n",
            "│ Gemm            │ 15             │ 15               │\n",
            "│ Greater         │ 3              │ 3                │\n",
            "│ GreaterOrEqual  │ 1              │ 1                │\n",
            "│ MatMul          │ 49             │ \u001b[1;32m48              \u001b[0m │\n",
            "│ Mul             │ 62             │ \u001b[1;32m53              \u001b[0m │\n",
            "│ Neg             │ 1              │ 1                │\n",
            "│ Not             │ 1              │ 1                │\n",
            "│ OneHot          │ 1              │ 1                │\n",
            "│ Pow             │ 12             │ 12               │\n",
            "│ Range           │ 5              │ 5                │\n",
            "│ ReduceMean      │ 24             │ 24               │\n",
            "│ ReduceSum       │ 2              │ 2                │\n",
            "│ Relu            │ 12             │ 12               │\n",
            "│ Reshape         │ 48             │ \u001b[1;32m45              \u001b[0m │\n",
            "│ Resize          │ 1              │ 1                │\n",
            "│ ScatterND       │ 2              │ 2                │\n",
            "│ Shape           │ 141            │ \u001b[1;32m57              \u001b[0m │\n",
            "│ Sigmoid         │ 1              │ 1                │\n",
            "│ Sin             │ 2              │ \u001b[1;32m1               \u001b[0m │\n",
            "│ Slice           │ 21             │ \u001b[1;32m17              \u001b[0m │\n",
            "│ Softmax         │ 7              │ 7                │\n",
            "│ Sqrt            │ 33             │ \u001b[1;32m26              \u001b[0m │\n",
            "│ Sub             │ 14             │ 14               │\n",
            "│ Tile            │ 1              │ 1                │\n",
            "│ Transpose       │ 32             │ \u001b[1;32m31              \u001b[0m │\n",
            "│ Unsqueeze       │ 116            │ \u001b[1;32m83              \u001b[0m │\n",
            "│ Where           │ 10             │ \u001b[1;32m6               \u001b[0m │\n",
            "│ Model Size      │ 15.8MiB        │ 19.7MiB          │\n",
            "└─────────────────┴────────────────┴──────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional, mount GDrive for faster model download (Copy it to your Google Drive and then download)"
      ],
      "metadata": {
        "id": "JxyJ9H5xjoTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "metadata": {
        "id": "J6ameAOEjm9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84552cb0-1d40-4ea2-9d11-16d8c4771887"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/segment-anything-2/\n",
        "!cp {model_type}_encoder.onnx '/content/gdrive/My Drive/'\n",
        "!cp decoder.onnx '/content/gdrive/My Drive/'"
      ],
      "metadata": {
        "id": "ZyBDSz5RjAb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d5d582-ce79-4bd8-f350-c5ffa74c2813"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/segment-anything-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cm1MTm1RNHJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}